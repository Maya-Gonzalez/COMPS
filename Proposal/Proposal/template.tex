\documentclass[10pt,twocolumn]{article} 

\usepackage{oxycomps} % use the main oxycomps style file

\bibliography{references}

\pdfinfo{
    /Title (Using ML to Detect and Classify Alzheimer's Data)
    /Author (Maya Gonzalez)
}

\title{Using ML to Detect and Classify Alzheimer's Data}

\author{Maya Gonzalez}
\affiliation{Occidental College}
\email{mgonzalez3@oxy.edu}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\subsection{Goals}

\subsection{Audience}

\subsection{Requirements}

\section{Sections of the Oxy CS Comps Paper}

\subsection{Introduction and Background}

\subsection{Problem Context}
Alzheimer's disease dramatically effects the individual themselves along with family. There is currently no cure, only measures that can slow the progression. Often times, the main indication is the recognition of memory loss and differences in environment interactions. It is also possible to analyze various brain scans and clinical data to determine the extent of a person's  condition. The current process requires expensive scans, a specialized medical expert to analyze the images and overall takes time. 


Among other factors, loss of brain mass is one indicator of Alzheimer's Disease (AD), yet it is unclear exactly at what stage loss of mass takes place. This question is out of the scope for this project, but may be critical in understanding if detecting these anatomical changes can prevent early onset Alzheimer's or simply be used to diagnose. 

Machine learning (ML) has been utilized in the medical field for various applications due to its ability to reach accurate conclusions in complex problem spaces. One such application is in classifying and predicting Alzheimer’s disease (AD) from brain scans and other clinical data. This has shown promising results in the detection of brain changes that are the result of dementia and the classification of Alzheimer's disease and other brain conditions. 

Brain scans have been a popular modality to study and train a ML model on due to the physical indicators of cognitive impairment. MRIs are widely used for general medical imaging and are suitable for brain studies scans due to its noninvasive nature. The clarity and detail of MRI's make them an advantageous tool in early detection and diagnosis of brain related conditions, specifically Alzheimer's. 3-D MRIs can show subtle anatomical changes that indicate brain atrophy from early stages of Alzheimer’s disease. It is also important to note that there are various other conditions that may have similar or overlapping symptoms. Additional methods must be used to confirm AD or rule out other causes of dementia.

Magnetic Resonance Imaging (MRI) is widely used for general medical imaging and is suitable for brain studies scans due to it's noninvasive nature. MRI scans use radio waves and magnetic fields to produce a detailed image of the brain structure and the brain regions. The clarity and detail of MRI's make them an advantageous tool in early detection and diagnosis of brain related conditions, specifically Alzheimer's. Popular data sets include Alzheimer's Disease Neuroimaging Initiative (ADNI) and Open Access Series of Imaging Studies (OASIS). The number of patients represented in each data set varies, but it is generally less than 1,000 total patients. The limited amount of data available to train on poses performance issues for the classification of AD. 

I am interested in learning about and gaining more experience with machine learning algorithms. Aside from my Machine Learning course, I do not have experience deciding upon and implementing ML algorithms to solve a real problem. 

Due to it's success with image processing, I will be building a Convolutional Neural Network that can classify brain scans as having Alzheimer's or not. I will explore implementing various techniques that can improve the accuracy and other achievement metrics. Based on the findings of multi-modality work, I will be supplementing the image data with additional demographics and clinical data to increase the performance of classification. I will be using a widely used data set, Alzheimer's Disease Neuroimaging Initiative (ADNI). I would be interested in exploring the possibility of testing the built model on other data sets. 
\subsection{Technical Background}

In what is known as End-to-End Learning, Deep Learning (DL) models are able to use raw data as input, which enables features to automatically be learned. It also circumvents the need for specialized doctors to label datasets. One such DL model is Convolutional Neural Networks (CNNs). Although they were designed for general image usage, CNNs have become more popular in the medical imaging analysis research community. The goal of a CNN in image processing is to reduce the image dimension in such a way that avoids losing important information but reduces the pixel amount to reduce computational power. Relative to other ML models, CNNs have the ability to reach higher levels of abstraction and complexity, which enables them to detect subtle, scattered, and complex patterns in the data. CNNs have seen better results on image classification tasks compared to traditional ML methods.

An image is fed into the CNN as input and processed by each pixel value. A filter that is smaller then the image size passes over the image a set number of times. Each pixel in the image is multiplied by the corresponding pixel value of the filter. The sum of all computed values is saved in the output channel map. The goal of this step is to extract high-level features such as edges from the input image. 

CNNs typically have more than one layer and must have a way to take the activations of one layer and map those to the activations of the next layer. The way this is done in CNNs is through forward propagation and back propagation which uses a convolution operation. The forward pass involves derivation of the gradients moving from left to right. At the end of the circuit, the loss is computed by using the loss function. Relating back to the filter, the convolution between the filter and the input derives the output. Backpropagation is used to derive the error between the expected output and the network’s output. It measures the influence of every intermediary value in the graph on the final loss function. After the forward path, the backward path is derived by calculating the gradient of all the intermaries in the circuit from right to left until we are left with the gradients of the input. Since this is a recursive function, in the base case we consider the identity function, the gradient of f with respect to f, which has a gradient of 1. To compute the gradient of the input layer, the chain rule must be applied. Each intermediate value is positively influencing the loss if it is a positive gradient, whereas it is negatively influencing the loss if it is a negative gradient. The general backpropagation procedure calculates the error gap, changes the weights, and stops once the gap is no longer updated or when the differential value becomes 0. The least squares method can be used to calculate the error value. A gradient descent function is used to calculate the weights of each layer. Training under gradient descent forces the network to learn to extract the most important features or features that will minimize the loss. 

\subsection{Prior Work}

There are various methods to extract valuable information from MRI images. The main methods include voxel-based features, region-of-interest based features, and whole-image-based features. The most popular methods utilized in this research area include Singular Value Decomposition (SVD), Principal Component Analysis (PCA), and Convolution Neural Networks (CNN). Regardless of the chosen model, researchers experiment with data augmentation, and other methods to reduce overfitting. Data augmentation involves generating reflections and transformations of images. The network then makes a prediction on all these images. Another form of data augmentation is to alter the RGB intensities.  

Although current research has shown promising results and some exhibit near perfect accuracy in ML models, ML is not widely used in actual clinical practice. This is partly due to the fact that most models are trained on one type of data, such as images, whereas there are additional factors that need to be considered in a real patient’s case. Most existing regression methods only utilize a single modality of data without taking into account related information derived from different modalities. Specifically, most studies solely focus on disease classification or clinical score regression. There have been some researchers creating models that account for multiple modalities of data and have found interrelationships between the different types of data. These researchers have described the limitations in work that only considers low-level features such as gray matter tissue volumes from MRI data. There have been efforts to address both these tasks in a unified methodology. Such studies have found the features from both tasks are highly related as described in Zhang et. al \cite{zhang2014classification}, Liu et al. \cite{liu2018joint}, and Suk et al.\cite{suk2013deep}. The multi-modality methods have generally found improved classification performance of AD. 

Convolution Neural Networks have shown state-of-the-art performance in image classification problems, and specifically Alzheimer's Disease classification. 

Krizhevsky et al.\cite{krizhevsky2012imagenet} 
created a deep CNN, AlexNet, that has outperformed other top models, achieved record error rates, and won the ImageNet classification challenge in 2012. The architecture has 60 million parameters,  650,000 neurons, five convolutional layers, and three fully-connected layers. The high performance is partially credited to the depth of the model. They found that removing any layers resulted in degraded performance of the model. They implemented non-saturating nonlinearity to reduce the training time required to reach 25\% training error, which was critical to training on a large data set.  AlexNet also includes overlapping pooling, which is not a traditional method but was found to further reduce the error rate. The authors found that it is more difficult to overfit in methods with overlapping implemented. Moreover, two methods were implemented to reduce overfitting: data augmentation and dropout. Data augmentation included image translations and horizontal reflections. Although this method makes the training set larger by an exponent of 2048, it dramatically reduces overfitting to allow for the use of a large network. Their results broke previous state-of-the-art records in competitions, as they achieved top-1 and top-5 error rates of 37.5\% and 17.0\%, respectively. 

Some researchers attempt to improve accuracy metrics by implementing well-researched methods such as data augmentation and multi-modality training. Yoon et al. \cite{yoon2018classification} achieved 98\% overall accuracy by using AlexNet and Mini-batch Stochastic Gradient Descent. The researchers also explored data augmentation through rotations, which resulted in further improved accuracy. They concluded that their method was a good model for classifying NC, MCI, and AD in 18F-FBB amyloid PET brain images. Liu et al. \cite{liu2018joint} developed a convolutional neural network, DM2L, that incorporates both feature extraction and classification model training. Their joint classification and regression learning framework has an advantage as they incorporated patient's demographic information (age, gender, and education) into the learning process. The DM2L method performs better when compared to similar joint classification approaches. The researchers credit the superior performance to the inclusion of subject's demographic information as well as the simultaneous learning of MRI features along with the classifier and regressor.



\subsection{Methods}
The approach to the project is clearly laid out, and justified with respect to literature and the goals of the project. Alternate approaches are discussed and the reasons for not attempting them delineated.


decide on the model's architecture 
I will build the CNN 
based on the performance, I will adjust the architecture. 
\subsection{Evaluation}
The metrics used and method of collection are clearly explained, and are justified with respect to the literature and the goal of the project. Other metrics are considered, and the reasons for not using them delineated.



I will evaluate based on a set of metrics. I will compare my results from these chosen metrics to similar reserach. 
\subsection{Ethical Considerations}
There are various ethical concerns to consider when incorporating a Machine Learning (ML) model into the decision making process of a real clinical scenario. Using ML to diagnose Alzheimer’s disease (AD) from image data is one problem space that has the potential to prevent unnecessary invasive procedures or to diagnose a patient before symptoms appear in hopes to slow down the progression. The concerns that are arguably most important are the ones concerning the safety of the patient. There are many factors and decisions being made in the overall process even before a patient is introduced into the equation. The general process includes data collection, research and model building, and then implementation into a clinical setting. It is necessary to rule out all possibilities of introducing a bias into the model before any ML model can be used in the decision making process of a real case. If not, these biases will be encoded into the final decision and can have life threatening consequences.

To start off at the beginning of the process, the data sets available are too small for training purposes. Current research mainly trains on one of several popular data sets, which contain only several hundreds of participants. It is also important to consider the specific data each model is trained on, as the model may become highly dependent on the data trained on. The model may perform well on a certain population group if that was the only data provided for training. It is acknowledged that most medical datasets are trained on a subset or specific group of the general population. The lack of racial and ethnic diversity in the participants that make up a dataset is a problem that extends into the space of brain scan databases. For example, the ADNI dataset is one of the largest AD datasets available and widely used in research. However, the participant data is biased, as demonstrated by the participant makeup. According to the reported ADNI participant demographics \cite{adni}, of the 822 total subjects sampled from, 764 identify as White. Only 39 of the total participants identified as black or African American and 14 identified as Asian. Of the 822 participants, 21 identified as Hispanic or Latino. Males accounted for 478 of the participants while females made up 344 of the total participants. The lack of participant sampling which accurately reflects the general population reduces the ability to generalize the models trained on these data sets. 

Regardless of the model’s performance, there is a broader question of how the model would be implemented in a medical setting. More specifically, at what stage would the model’s decision be incorporated? How much influence should the decision of a model have over that of the medical professionals? Most medical professionals and doctors involved in the decision making process may not have experience or familiarity with ML. They may see this tool as a black box, in that they have no idea how the decision is being reached and therefore may not trust the ML model. This may lead medical practitioners to adopt a biased view of the model's predictions. 

Once deployed into a clinical setting, another issue emerges of who has access to this technology. MRI scans are an expensive procedure in the US healthcare system, so only individuals with the available money would be able to take advantage of this resource. This lack of accessibility further deepens the barrier of access to medical innovations. If a tool is only able to help a subset of the general population who have the available resources, then a question arises of how useful this will actually be.

Due to the outlined issues, it is not yet ethically feasible to use a ML model to assist in the decision making process of Alzheimer’s disease. It is not worth risking a patient’s life to make wishful strides in Machine Learning. 
\subsection{Timeline}
My current plans are to spend the summer finalizing the exact method and implementation that I would like to pursue. In the fall, I can focus on optimizing the build model and working on any lasting details. Specifically, by June I would like to finalize the exact model that I will be building and decide on any tools or packages I will be using. Starting in June I will begin building the implementation. BY the end of August, I will have a complete model built and running. I will spend the first half of August analyzing the results and reviewing literature or similar work. I will then take the rest of August and September to implement any optimizations that I decided on from my previous research. In October, I will finalize my overall implementation and begin creating my final deliverables. The final deliverables include the working code, documentation, the poster, a poster presentation, and my final proposal. In November I will begin practicing for the poster presentation. 
\subsection{Limitations, Future Work, and Conclusion}

\subsection{Appendices}

\section{Conclusion}

\printbibliography 

\end{document}
